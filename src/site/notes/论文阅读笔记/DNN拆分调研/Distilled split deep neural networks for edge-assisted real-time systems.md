---
{"dg-publish":true,"permalink":"/论文阅读笔记/DNN拆分调研/Distilled split deep neural networks for edge-assisted real-time systems/"}
---

# Distilled split deep neural networks for edge-assisted real-time systems
DOI：[Fetching Title#kb6b](https://doi.org/10.1145/3349614.3356022)
发表年份：2019
## Summary
该论文提出了一种框架，用于拆分用于图像处理的深度神经网络 (DNN)，并在各种网络条件和计算参数下最大限度地减少捕获到输出的延迟。核心思想是将 DNN 模型拆分为头部和尾部模型，这两个部分分别部署在移动设备和边缘服务器上。与先前介绍DNN拆分框架的文献不同，作者蒸馏了头部DNN，以降低其计算复杂性并引入瓶颈层，从而最大限度地减少移动设备的处理负载以及无线传输的数据量。在这些极端点运行不佳的条件下，拟议的方法代表了局部处理和边缘计算之间的中间选择。结果显示，与直接拆分相比，已用带宽减少了98％，计算负载减少了85％。
## Research Objective
1. 本文的核心目的是为实时图像分析应用提供了一种更加精细的拆分DNN模型和分配计算负载的方法。
2. **为了降低DNN拆分后部模型在移动设备上的计算复杂度**，对DNN模型的头部部分进行蒸馏。
3. **为了减少网络传输的数据量**，在蒸馏后的头部模型中引入瓶颈层。
## Background and Challenges
![Pasted image 20230728101516.png|700](/img/user/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/DNN%E6%8B%86%E5%88%86%E8%B0%83%E7%A0%94/_resource/Pasted%20image%2020230728101516.png)
1. DNN模型的复杂化：DNN模型的计算复杂度与其所解决的问题的复杂度一直在增长。例如，在图像分类领域，1998年提出的Le Net5 仅有7层，而2017年提出的Dense Net 有713层。尽管近年来嵌入式系统的发展，DNN模型在移动平台上的执行变得越来越困难，特别是对于任务关键或时间敏感的应用，有限的处理能力和能源供应可能会降低系统的响应时间和寿命。
2. Task Offloading：将数据处理任务卸载到边缘服务器，即位于网络边缘的可计算设备，已被证明是一种有效的策略，可以减轻移动设备的计算负担，并在一些应用中减少捕获到分类的输出延迟。然而，恶劣的信道条件，例如由于干扰、与其他数据流的竞争或信号传播的退化，可能会显著增加向边缘服务器发送信息丰富的数据所需的时间。 
3. DNN partition：将DNN模型拆分为头部和尾部，分别部署在移动设备和边缘服务器上，以优化处理负载分配。然而，由于用于图像处理的DNNs的结构特性，直接拆分的方法可能会导致很大一部分处理负载被推到移动设备上，同时也会导致网络上传输的数据量较大。 
4. 从图中可以看出，**层的输出只在后面的层中明显小于输入**。 也就是说，想要减少数据传输可能会导致端设备处理更复杂的模型，导致整体时间恶化。直觉上，这些趋势不允许在移动设备与边缘服务器相比具有更小的计算能力的非对称系统中实现有效的拆分策略。此外，传输数据所需时间的增加会惩罚相对于纯卸载。因此，我们认为在较弱的设备上的计算和可能受损的无线信道上的通信之间实现有利的平衡需要对DNN架构进行修改。
## Method
### network distillation
![Pasted image 20230728102224.png|250](/img/user/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/DNN%E6%8B%86%E5%88%86%E8%B0%83%E7%A0%94/_resource/Pasted%20image%2020230728102224.png)![Pasted image 20230728104303.png|250](/img/user/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/DNN%E6%8B%86%E5%88%86%E8%B0%83%E7%A0%94/_resource/Pasted%20image%2020230728104303.png)
在本文考虑的设定中，使用蒸馏的主要优点是：
- 适当蒸馏的模型往往具有相当的性能，同时减少了模型中使用的参数数量，从而降低了计算复杂度。
- 学生模型在蒸馏过程中往往避免过拟合，因为来自教师模型的软目标具有正则化效应。
- 如果分割点位于学生模型内部，那么学生模型中较小的节点数量会导致要传输到边缘服务器的数据自然减少。
- 学生模型的更易于管理的结构将允许插入激进的瓶颈层。

以DenseNet - 169为例，在第1、第2和第3个平均池化层(第14、30、66层)处识别原始DNN模型中的自然瓶颈点。利用我们提出的头网络蒸馏，我们在瓶颈点处拆分了一个DNN模型，并构建了一个不同的较小的学生模型，我们用它替换了原始的头网络，以降低移动设备上的计算复杂度。 
### bottlenecks
![Pasted image 20230728104355.png|300](/img/user/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/DNN%E6%8B%86%E5%88%86%E8%B0%83%E7%A0%94/_resource/Pasted%20image%2020230728104355.png)
瓶颈问题最近在理论上被证明可以促进DNN学习最优表示，从而实现模型内部的压缩。然而，如表1所示，直接对原始DNN模型中的"自然"瓶颈进行更激进的处理，即使在相对温和的压缩率下也会导致精度下降。此外，一些模型，如Inception - v3，没有给出任何候选分裂点。因此，需要对架构进行更大幅度的修改。

学生模型降低了复杂度，但仍然会产生大小至少为输入的30 %左右的输出。我们现在设计蒸馏学生模型，我们在其中引入激进的瓶颈，以进一步减少传输到边缘服务器的数据量。为此，在所有考虑的DNN中，我们在学生模型的早期阶段人为地注入瓶颈。我们强调，因此，分割点在学生模型内部，而不是在其末端，边缘服务器将需要执行学生模型的一部分。 

表3显示，即使在DNN模型(例如ResNet - 152和Inception - v3模型)没有瓶颈点的情况下，我们提出的方法能够在学生模型中引入一个激进的瓶颈，从而通过在该点拆分学生模型来降低移动设备上的计算复杂度和传输的数据量。与原始模型相比，我们实现了复杂度和输出数据规模的显著降低，准确率最多下降约1 %。
## Evaluation
### Enviroment Setting
![Pasted image 20230728104739.png|300](/img/user/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/DNN%E6%8B%86%E5%88%86%E8%B0%83%E7%A0%94/_resource/Pasted%20image%2020230728104739.png)
### Detail
- DDN模型：DenseNet-201
- 拆分并加入瓶颈后：Mimic w / B
- 对比了不同设备作为边缘服务器的情况
### Observation
![Pasted image 20230728105950.png|325](/img/user/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/DNN%E6%8B%86%E5%88%86%E8%B0%83%E7%A0%94/_resource/Pasted%20image%2020230728105950.png)
1. 图3 ( a )和( b )显示了当边缘服务器为笔记本电脑时，不同移动设备的增益变化趋势。 直观上，数据速率越大，相对于Org.ES的增益越小。因为相对于在较慢的平台上执行时间的增加，超过瓶颈授予的τ数据减少量减少所带来的收益。
2. 具有强大移动设备的配置突出了Mimic w / B复杂度的普遍降低，即使在信道容量较高的情况下也能带来可观的增益。 
## Conclusion
本文提出了一种有效分割边缘辅助系统中DNN的方法。我们的技术基于蒸馏，将其应用于分割模型的头部部分。然后对蒸馏头模型进行修正，引入瓶颈。进一步的研究是必要的，以了解我们的方法的一般含义。然而，直觉表明，它可以大致对应于自编码器直接将DNN的输入映射到后一层输入的一种特殊情况。基于学生-教师的方法允许构建具有攻击性瓶颈的小模型。在真实的嵌入式计算机上的结果确定了所提技术有效的通信速率范围。 
## Note

